---
title: "Hotel Time Series Analysis"
author: "Luis Martinez"
output: 
  github_document:
    fig_width: 5
    fig_height: 5
    dev: jpeg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.path = "my_images/"
)
```
# Problem Frame Work

This is a city hotel that counts with 220 rooms(this is a rough estimate, since the data does not state a specific number of rooms), we will try to forecast with maximum accuracy using different models for long term forecasting.

We will not use the traditional Pick Up methods, since the seasonality is aggregated differently, however this is for forecasting weeks in advance.

# Overview of the Data

This data was imported from Kaggle under hotel booking.

It is important to note that prior to analysis this data was cleaned using Python, since this documentation is for modeling we skip that part, however it will be appended in the repository.

For time series modeling R is most robust with it's forecast library.

In this document we will explore:
1. Linear Regression (Additive and Multiplicative)
2. SARIMA (Auto Regression Models and Moving Average)
3. Dynamic Regression
4. Ensemble Models
```{r, message=FALSE, warning=FALSE, echo=FALSE}

library(forecast)
library(tidyverse)
library(lubridate)
library(zoo)
``` 
```{r, echo=FALSE}
data = read.csv("C:\\Users\\luism\\Coding\\REV-M\\projects\\RevenueManagement\\hotel_timeseries.csv")
data = data %>% 
  rename(Reservations = X0)
head(data)
```

## Data preparation
As we can see we have daily reservations for each day, for this we will aggregate the results to weekly data so we can compile it and it would be easier to see the trends.



For this exercise we will se the total reservations we had for a week, then we can build a more complicated model using days of the week and months.
```{r}
data$Date <- as.Date(data$Date)
weekly_data <- data %>% 
  mutate(WeekStart = floor_date(Date, "week")) %>% 
    group_by(WeekStart) %>% 
      summarise(ReservationsWeek = sum(Reservations),
                .groups = 'drop')

# Extract min year and week day

start_year <- year(min(weekly_data$WeekStart))
start_week <- isoweek(min(weekly_data$WeekStart))

data_ts <- ts(weekly_data$ReservationsWeek, start = c(start_year, start_week), frequency = 52)

capacity <- 220 * 7
```

## First Vizz
Let's take a first look at how this data looks like and make ourselves an idea with what we are working.

```{r}

plot(
  data_ts,
  main = 'Hotel Weekly Sales',
  xlab = 'Time',
  ylab = 'Reservations',
  bty = 'n',
  col = 'black',
  ylim = c(0, 1600)
)
abline(h = capacity, col = 'red', lty = 2)

legend(
  "bottomright",
  legend = c("Reservations", "Capacity"),
  col = c("black", "red"),
  lty = c(1,2),
  bty = 'n'
)

```

The first thing that we can observe is a strong volatility of this hotel reservations through the year.

There seems to be a slight seasonality at the end of the year, as we can see at the end of the year there seems to be a dip, followed by a spike for end of year holidays.

Then through the years we can observe that in the mid year is pretty difficult to observe a pattern but there is a slight increasing trend in the years.

## Seasonal Overview

```{r}
seasonplot(data_ts,
           main = 'Weekly Seasonality',
           xlab = 'Week',
           ylab = 'Total Reservations',
           bty = 'n',
           ylim = c(0,1600))
```

Now we can observe more clearly that spike in the last weeks going 400 above average, however we can also see the downward spike for the first weeks of the year, then through the middle of the year it appears to be very random, and no clear seasonality so far.

## AutoRegression
```{r}
Acf(data_ts, bty= 'n')
```

With this lag autocorrelation graph, we can clearly see a long dependency in what happened in the weeks before. It also reasures the seasonality as we can see that there are some spikes that switch from up to down.

# Modeling

## Linear Regression

First we will not utilize Cross Validation but a simple train and test split, we will use CV for the best model of each section of the models.

Let's start with a simple linear regression with seasonal dummy variables.

```{r}
n <- length(data_ts)
n_Valid <- 10
n_Train <- n - n_Valid

train_ts <- window(data_ts, start = c(start_year, start_week), end = c(start_year, n_Train))
valid_ts <- window(data_ts, start = c(start_year, n_Train + 1))
```

```{r}
model_1 <- tslm(train_ts ~ trend + season)
summary(model_1)
```
We can see that our model explains fairly well with a R-squared of 0.9353 and for being a baseline it is a great start!

What if we use a fourier series to find seasonality instead of dummy variables.

```{r}
model_2 <- tslm(train_ts ~ trend + season, lambda = 0)
summary(model_2)
```

The problem with this model is that it is very hard to interpret the results as for how the seasonality works, but we can plot both models out and see how they fit.

```{r}
plot(
  train_ts,
  main = "Fit of Linear Regression Models",
  xlab = "Time",
  ylab = "Total Reservations",
  bty = 'n',
  ylim = range(c(train_ts, model_1$fitted.values, model_2$fitted.values)),
  col = 'black',
  lwd = 2
)
lines(model_1$fitted.values, col = 'cyan2')
lines(model_2$fitted.values, col = 'darkorange')
legend(
  'topleft',
  legend = c("Training", "Additive Model", "Log-Scale Model"),
  col = c("black", "cyan2", "darkorange"),
  lwd = c(2,1,1),
  bty = 'n',
  cex = 0.8
)
```

We can clearly see that the red line closely resembles the training data with the values in the middle being exactly the same! IN the multiplicative we can see that both have the same results, just that on the peaks the multiplicative tends to overestimate or underestimate, being a more aggressive model.

Now lets see how they perform on unseen data.

```{r}
model_1_pred <- forecast(model_1, h = 35, level = 90)
model_2_pred <- forecast(model_2, h = 35, level = 90)
```
**Accuracy for Additive Model**
```{R}
accuracy(model_1_pred$mean, valid_ts)
```
**Accuracy for Log-Scale Model**
```{r}
accuracy(model_2_pred$mean, valid_ts)
```

We can see that our model is constantly over estimating the forecast, and lacks to capture the downward trend that the whole time series shows, let's plot it to be able to see it better.

```{r}
plot(model_1_pred,
     main = "Linear Regression Forecast",
     xlab = 'Time',
     ylab = 'Total Reservations',
     bty = 'n',
)
lines(
  model_2_pred$mean, col = "red2"
)
lines(
  valid_ts, col = 'black', lwd = 2
)
legend(
  "topleft",
  legend = c("Linear model", 'Log Scale', 'Actual'),
  col = c('blue', 'red2', 'black'),
  lwd = c(1,1,2),
  bty = 'n',
  cex = 0.8
)
```


Overall the model is not great, we overestimate. We are still over the limit of the confidence level however, this problem begs another model more complicated that this one,


## ARIMA and SARIMA MODEL

Let's first explore how SARIMA models function. 

They use differentiation to take non-stationary time series into stationary and then find the number of lags of trend and shock (error) to correlate and average respectively. 

We can first try using what the R package suggests also by doing both the additive forecast and the multiplicative in the log-scale.

**Let's Use R's suggestion**

```{r}
model_3 <- auto.arima(train_ts) #Additive
model_4 <- auto.arima(train_ts, lambda = 0)
```

**This is the additive model output**
```{r}
#| echo: false

model_3
```

We can see that we take one difference and then only use the last error shock term to perform the moving average by the coefficient.

**THis is the multiplicative model output**
```{r}
model_4
```
In this model we do take one difference and we use the last two lags to auto regress the forecast. This means that this week's sales depends on the past two, mostly negatively.


Let's see graphically how it fits.

```{r}
plot(
  train_ts,
  main = "Fit of ARIMA Models",
  xlab = "Time",
  ylab = "Total Reservations",
  bty = 'n',
  ylim = c(0, 1700),
  col = 'black',
  lwd = 2
)
lines(model_3$fitted, col = 'cyan2')
lines(model_4$fitted, col = 'darkorange')
legend(
  'topleft',
  legend = c("Training", "ARIMA ADD", "ARIMA LOG"),
  col = c("black", "cyan2", "darkorange"),
  lwd = c(2,1,1),
  bty = 'n',
  cex = 0.8
)
```

We can see that both lines tend to capture the signal very well of the model but it has a very hard time to understand the peak that it experiences in the last two weeks of the year. Let's see how the forecast of this models perform.

```{r}
model_3_pred <- forecast(model_3, h = 35, level = 90)
model_4_pred <- forecast(model_3_pred, h = 35, level = 90)
```

**The ARMA (Additive) model**
```{r}
#| echo: false
accuracy(model_3_pred, valid_ts)
```


**The ARMA (Multiplicative Model)**

```{r}
#| echo: false
accuracy(model_4_pred, valid_ts)
```
The first thing that pops to mind is that this model is constantly underestimating the validation set, we can see that because the ME is positive, different from the regression models. The percentage error is still in the 20's% and in the regression model it was 20's for additive and 30's for multiplicative.

However if we look at the values of the prediction:
```{r}
head(model_3_pred$mean)
```

We can see that all values are the same, this is a clear signal of a random walk behavior! Meaning that our model just stays on the mean as its best guess and this happens with the multiplicative as well.

This kind of time series models are very hard to forecast, another example of random walks are stocks!

For this reason we might want to use a more robust model.

## DYNAMIC REGRESSION

This is one of the most famous forecasting models for time series data, used by many government agencies and statistics departments. It used two of the models we have already used: regression and SARIMA.

This will use the trend and level as a regression model but the error as a autoregressive model.

Let's model it!

```{r}
xreg <- cbind(trend = seq_along(train_ts), seasonaldummy(train_ts))
model_5 <- auto.arima(train_ts, xreg = xreg)
model_5
```
We can clearly observe in this model that the error terms have a correlation term but no difference. Let's take a look at the multiplicative with a log scale.

```{r}
model_6 <- auto.arima(train_ts, xreg= xreg, lambda = 0)
model_6
```

The curious thing about this two models is that the error term for regression is different, but we do know that we take more than 50% of our previous error. Let's see graphically how it fits.

```{r}
plot(
  train_ts,
  main = "Fit of Dynamic Regression",
  xlab = "Time",
  ylab = "Total Reservations",
  bty = 'n',
  ylim = c(0, 1700),
  col = 'black',
  lwd = 2
)
lines(model_5$fitted, col = 'cyan2')
lines(model_6$fitted, col = 'darkorange')
legend(
  'topleft',
  legend = c("Training", "ARIMAX ADD", "ARIMAX LOG"),
  col = c("black", "cyan2", "darkorange"),
  lwd = c(2,1,1),
  bty = 'n',
  cex = 0.8
)
```

These two models seem to fit way better the training data but we still need to find out how it performs in the forecasting and validation set.

```{r}
newxreg <- cbind(trend = (n + 1): (n + 35), seasonaldummy(valid_ts, h=35))
model_5_pred <- forecast(model_5, xreg = newxreg, level = 90)
model_6_pred <- forecast(model_6, xreg = newxreg, level = 90)
```

**Result of additive of dynamic regression model**

```{r}
accuracy(model_5_pred, valid_ts)
```

**Result of multiplicative dynamic regression model**
```{r}
accuracy(model_6_pred, valid_ts)
```


We see that both our models perform not so greatly in the validation test, this could be due to two reasons, first one is the lack of the data, since we only have one year and a half in order to forecast.

This can be seen to the great performance for training the data but lacking in the test data, this is particularly common for very scarce data.

## NEURAL NETWORK
We can also create a neural network in order to create a more robust model.

We will use a small hidden layer and hidden size in order to save computational power and time.

We will use 2 non seasonal lags and 1 seasonal lag and 5 hidden neurons in the hidden layer.


```{r}
model7 <- nnetar(train_ts, p = 2, P = 1, size = 5, xreg = xreg)

plot(train_ts,
     lwd = 2,
     bty = 'n',
     main = "Neural Network Model Fit",
     xlab = 'Time',
     ylab = 'Reservations')
lines(model7$fitted,
      col = 'red3')
legend('topleft',
       legend = c("Training", "Neural Network"),
       col = c("black", "red3"),
       lwd = c(1,2))
```

Although the Neural Network fits perfectly, we see that it needs a lot of data before it can start training, therefore we will not include in in our ensemble model.


## ENSEMBLE MODELS

Now that we have created 6 different models, we can create one ensemble model by combining all of our models and try to fit the validation set as better as wen can.

In the last models we have been using one validation and one training set, however that has not yielded the best results, therefore we are going to use Cross Validation, meaning creating multiple training and validation data sets to find the minimum error our ensemble model will have.

Since we have 114 weeks we are going to create a training window size of 52 and forecast the next 13 weeks (meaning the next quarter).


First, we need to create the training window and the test, this is for a sliding cross validation. Here we will use 52 weeks or one year of data to predit the next quarter
```{r}
n <- length(data_ts) 
h <- 13 
w <- 52 


start_pos <- w
end_pos <- n - h


xreg_total_fourier <- cbind(trend = seq_along(data_ts), fourier(data_ts, K=15))
time_total <- time(data_ts)
```

Due to the limited data, since we need at least 3 years to be able to make good seasonal dummies, we will use fourier series in order to recreate the seasonality.

```{r, eval=FALSE}
CV_result <- c() ####Create matrix to store the results ###



for(t in start_pos:end_pos){
  print(paste("Iteration:", t))
  

  tmp_train <- window(data_ts, start = time_total[t - w + 1], end = time_total[t])
  tmp_test  <- window(data_ts, start = time_total[t + 1], end = time_total[t + h])
  
  #####Create the fourier series for the ARIMAX models####
  tmp_train_xreg_fourier <- xreg_total_fourier[(t - w + 1):t, , drop = FALSE]
  tmp_test_xreg_fourier  <- xreg_total_fourier[(t + 1):(t + h), , drop = FALSE]
  

  fourier_terms <- fourier(tmp_train, K = 5) #####For Linear Regression###
  tmp_model1 <- tslm(tmp_train ~ trend + fourier_terms)
  tmp_model2 <- tslm(tmp_train ~ trend + fourier_terms, lambda = 0)
  tmp_model3 <- auto.arima(tmp_train)
  tmp_model4 <- auto.arima(tmp_train, lambda = 0)
  tmp_model5 <- auto.arima(tmp_train, xreg = tmp_train_xreg_fourier)
  

  future_fourier <- fourier(tmp_train, K = 5, h = h)
  pred_1 <- forecast(tmp_model1, newdata = data.frame(future_fourier), h = h)
  pred_2 <- forecast(tmp_model2, newdata = data.frame(future_fourier), h = h)
  pred_3 <- forecast(tmp_model3, h = h) 
  pred_4 <- forecast(tmp_model4, h = h)
  pred_5 <- forecast(tmp_model5, xreg = tmp_test_xreg_fourier, h = h)

  

  tmp_cv_result <- c(
    Actual  = tmp_test[h],
    Pred_M1 = pred_1$mean[h],
    Pred_M2 = pred_2$mean[h],
    Pred_M3 = pred_3$mean[h],
    Pred_M4 = pred_4$mean[h],
    Pred_M5 = pred_5$mean[h]
  )
  
  CV_result <- rbind(CV_result, tmp_cv_result)
}

# Add descriptive column names
colnames(CV_result) <- c("Actual", "Pred_TSLM_Fourier", "Pred_TSLM_Fourier_Log", 
                         "Pred_ARIMA", "Pred_ARIMA_Log", "Pred_ARIMAX_Full")

# Final result table
CV_result
```
```{r}
#| echo: false
CV_result <- readRDS("cross_validation_result.rds")
```

After more than 100 iterations we can see how the actual and the different models vs the actual.
Here are the results of all the models with the actuals.

```{r echo=FALSE}
df_results <- as.data.frame(CV_result)

df_results %>% 
  select(-Ensemble_Mean, -Ensemble_Mean_WeightedMean)
```



In the ensemble model we can do it two ways, via mean and via a weighted mean as the best model from all the models. We will apply both.

**ENSEMBLE WITH MEAN**
```{r, echo=TRUE, eval=FALSE}
ensemnle_mean <- apply(CV_result[,c(2:6)], 1, mean)
CV_result <- cbind(CV_result, ensemnle_mean)
colnames(CV_result)[ncol(CV_result)] <- "Ensemble_Mean"
```

**ENSEMBLE WITH WEIGHTED MEAN VIA NNLS**
```{r, eval=FALSE}
library(nnls)
nnls_result <- nnls(A=CV_result[,2:6], b=CV_result[,1])
colnames(CV_result)[which(coef(nnls_result)>0.05)+1]
nnls_selected_model <- which(coef(nnls_result)>0.05)+1

ensemble_nnls <- apply(CV_result[,nnls_selected_model], 1, mean)

CV_result <- cbind(CV_result, ensemble_nnls)
colnames(CV_result)[ncol(CV_result)] <- "Ensemble_Mean_WeightedMean"
```


Lets take a look at the result of both ensemble models
```{r, echo=FALSE}
df_results %>% 
  select(Actual, Ensemble_Mean, Ensemble_Mean_WeightedMean)
```
We can see that precision wise, the ensemble models do a great job at capturing the signal, even without the necessary data to create better results.

Let's see the accuracy of the models.

**Accuracy Mean Ensemble Model**
```{r, echo=FALSE}
accuracy(df_results$Ensemble_Mean, df_results$Actual)
```

**Accuracy NNL Weighted Mean**

```{r, echo=FALSE}
accuracy(df_results$Ensemble_Mean_WeightedMean, df_results$Actual)
```


Given the data the most accurate model is the Weighted Mean for a little percentage, overestimating the predictions.

Let's see how they fit in the graph.

```{r}
time_span <- 0:50
plot(
  CV_result[time_span,1],
  main = "Ensemble Models Performace",
  xlab = 'Test Sets CV',
  ylab = 'Weekly Reservations',
  type = 'l',
  lwd = 2,
  col = 'black',
  bty = 'n',
  ylim = c(0,1700)
  )
lines(CV_result[time_span,7], col='cyan2')
lines(CV_result[time_span,8], col='darkorange')
legend(
  'bottomright',
  legend = c('Actual', 'Mean_Ensemble', 'Weghted_Ensemble'),
  col = c('black', 'cyan2','darkorange'),
  lwd = c(2,1,1),
  bty = 'n',
  cex = 0.8
)
```

We can clearly see that our Ensemble Models have very wild guesses for the first data points, but seems to rectify themselves as more data is available and our base models seem to perform better.

```{r}
percentage_error <- abs( (CV_result[, "Actual"] - CV_result[, "Ensemble_Mean"]) ) / CV_result[, "Actual"]

CV_result <- cbind(CV_result, percentage_error)

colnames(CV_result)[9] <- "percentage_error1"

percentage_error2 <- abs( (CV_result[, "Actual"] - CV_result[, "Ensemble_Mean_WeightedMean"]) ) / CV_result[, "Actual"]

CV_result <- cbind(CV_result, percentage_error2)

colnames(CV_result)[10] <- "percentage_error2"

```


```{r}
plot(
  CV_result[time_span, 9],
  main = "Percentage Error of the Ensemble Models",
  col = 'cyan3',
  xlab = 'CV Test',
  ylab = 'Reservations',
  bty = 'n',
  type = 'l',
  lwd = 2
)
lines(
  CV_result[time_span, 10], col = 'darkorange', lwd = 2
)
legend(
  'topright',
  legend = c('Ensemble Mean', 'Weighted Mean Ensemble'),
  col = c('cyan3', 'darkorange'),
  lty = c(1,1),
  bty = 'n',
  cex = 0.8
)
```

# Next Steps

We can clearly see the power of Ensemble models, but in this particular problem we need more data in order to have a better result on unseen data since the model has to pick up the signal of seasonality with only one cycle, when at least we need 3 or 4.





